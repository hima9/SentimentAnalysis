{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# General\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport random\nimport os\nfrom os import path\nos.chdir('/kaggle/input/')\nfrom PIL import Image\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Set Plot Theme\nsns.set_palette([\n    \"#30a2da\",\n    \"#fc4f30\",\n    \"#e5ae38\",\n    \"#6d904f\",\n    \"#8b8b8b\",\n])\n# Alternate # plt.style.use('fivethirtyeight')\n\n# Pre-Processing\nimport string\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import PorterStemmer\n\n# Modeling\nimport statsmodels.api as sm\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nfrom nltk.util import ngrams\nfrom collections import Counter\nfrom gensim.models import word2vec\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(data.columns[0],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Title']=data['Title'].fillna('c')\ndata['Review Text']=data['Review Text'].fillna('c')\ndata['Division Name']=data['Division Name'].fillna('c')\ndata['Department Name']=data['Department Name'].fillna('c')\ndata['Class Name']=data['Class Name'].fillna('c')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#word length\ndata['word_count']=data['Review Text'].str.split().apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#character length\ndata['charac_count']=data['Review Text'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boolean for Positive and Negative Reviews\ndata[\"Label\"] = 0 #negative\ndata.loc[data.Rating >= 3,[\"Label\"]] = 1 #positive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_count = []\nfor x in data.columns:\n    unique_count.append([x,len(data[x].unique()),data[x].isnull().sum()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dataframe Dimension: {} Rows, {} Columns\".format(*data.shape))\npd.DataFrame(unique_count, columns=[\"Column\",\"Unique\",\"Missing\"]).set_index(\"Column\").T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T.drop(\"count\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[[\"Title\", \"Division Name\",\"Department Name\",\"Class Name\"]].describe().T.drop(\"count\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Continous Distributions\nf, ax = plt.subplots(1,3,figsize=(12,4), sharey=False)\nsns.distplot(data.Age, ax=ax[0])\nax[0].set_title(\"Age Distribution\")\nax[0].set_ylabel(\"Density\")\nsns.distplot(data[\"Positive Feedback Count\"], ax=ax[1])\nax[1].set_title(\"Positive Feedback Count Distribution\")\nsns.distplot(np.log10((data[\"Positive Feedback Count\"][data[\"Positive Feedback Count\"].notnull()]+1)), ax=ax[2])\nax[2].set_title(\"Positive Feedback Count Distribution\\n[Log 10]\")\nax[2].set_xlabel(\"Log Positive Feedback Count\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage Accumulation from \"Most Wealthy\"\ndef percentage_accumulation(series, percentage):\n    return (series.sort_values(ascending=False)\n            [:round(series.shape[0]*(percentage/100))]\n     .sum()/series\n     .sum()*100)\n# Gini Coefficient- Inequality Score\n# Source: https://planspace.org/2013/06/21/how-to-calculate-gini-coefficient-from-raw-data-in-python/\ndef gini(list_of_values):\n    sorted_list = sorted(list_of_values)\n    height, area = 0, 0\n    for value in sorted_list:\n        height += value\n        area += height - value / 2.\n    fair_area = height * len(list_of_values) / 2.\n    return (fair_area - area) / fair_area\n# Cumulative Percentage of Positive Feedback assigned Percent of Reviewers (from most wealthy)\ninequality = []\nfor x in list(range(100)):\n    inequality.append(percentage_accumulation(data[\"Positive Feedback Count\"], x))\n\n# Generic Matplotlib Plot\nplt.plot(inequality)\nplt.title(\"Percentage of Positive Feedback by Percentage of Reviews\")\nplt.xlabel(\"Review Percentile starting with Feedback\")\nplt.ylabel(\"Percent of Positive Feedback Received\")\nplt.axvline(x=20, c = \"r\")\nplt.axvline(x=53, c = \"g\")\nplt.axhline(y=78, c = \"y\")\nplt.axhline(y=100, c = \"b\", alpha=.3)\nplt.show()\n\n# 80-20 Rule Confirmation\nprint(\"{}% of Positive Feedback belongs to the top 20% of Reviews\".format(\n    round(percentage_accumulation(data[\"Positive Feedback Count\"], 20))))\n\n# Gini\nprint(\"\\nGini Coefficient: {}\".format(round(gini(data[\"Positive Feedback Count\"]),2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cumulative Percentage of Positive Feedback assigned Percent of Reviewers (from most wealthy)\ntop_20 = data[\"Positive Feedback Count\"].sort_values(ascending=False)[:round(data.shape[0]*(20/100))]\n\ninequality = []\nfor x in list(range(100)):\n    inequality.append(percentage_accumulation(top_20, x))\n\n# Generic Matplotlib Plot\nplt.plot(inequality)\nplt.title(\"Percentage of Positive Feedback by Percentage of Reviews\")\nplt.xlabel(\"Review Percentile starting with Feedback\")\nplt.ylabel(\"Percent of Positive Feedback Received\")\nplt.axvline(x=20, c = \"r\")\nplt.axhline(y=47, c = \"r\")\nplt.axhline(y=100, c = \"b\", alpha=.3)\n\nplt.show()\n\n# 80-20 Rule Confirmation\nprint(\"{}% of Positive Feedback belongs to the top 20% of Reviews\".format(\n    round(percentage_accumulation(top_20, 20))))\n\n# Gini\nprint(\"\\nGini Coefficient: {}\".format(round(gini(top_20),2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row_plots=[\"Division Name\",\"Department Name\"]\nf,axes=plt.subplots(1,len(row_plots),figsize=(14,4),sharex=False)\nfor count,eleinrow in enumerate(row_plots):\n    sns.countplot(y=eleinrow,data=data,order=data[eleinrow].value_counts().index,ax=axes[count])\n    axes[count].set_title(\"Count of Categories in {}\".format(eleinrow))\n    axes[count].set_xlabel(\"\")\n    axes[count].set_xlabel(\"Frequency Count\")\naxes[0].set_ylabel(\"Category\")\naxes[1].set_ylabel(\"\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clothing ID Category\nf, axes = plt.subplots(1,1, figsize=[14,7])\nnum=30\nsns.countplot(y=data[\"Clothing ID\"],data=data[data[\"Clothing ID\"].isin(data['Clothing ID'].value_counts()[:num].index)],\n              order=data['Clothing ID'].value_counts()[:num].index)\naxes[0].set_title(\"Frequency Count of Clothing ID\\nTop 30\")\naxes[0].set_xlabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['Clothing ID'].isin([1078,862,1094])].describe().T.drop('count',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the top 3 selling cloth id's,average age of buyers is around 43 and the average rating they give is 4.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data[\"Clothing ID\"].isin([1078,862,1094]),[\"Title\", \"Division Name\",\"Department Name\",\"Class Name\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data[\"Clothing ID\"].isin([1078,862,1094]),[\"Title\", \"Division Name\",\"Department Name\",\"Class Name\"]].describe().T.drop('count',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y=\"Class Name\",data=data,order=data['Class Name'].value_counts().index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_dtypes=['Rating','Recommended IND','Label']\nf,axes=plt.subplots(1,len(cat_dtypes),figsize=(14,4),sharex=False)\nincrement=0\nfor i in range(len(cat_dtypes)):\n    sns.countplot(x=cat_dtypes[increment],data=data,order=data[cat_dtypes[increment]].value_counts().index,ax=axes[i])\n    axes[i].set_title(\"Frequency Distribution for\\n{}\".format(cat_dtypes[increment]))\n    axes[i].set_ylabel(\"Occurrence\")\n    axes[i].set_xlabel(\"{}\".format(cat_dtypes[increment]))\n    increment += 1\naxes[1].set_ylabel(\"\")\naxes[2].set_ylabel(\"\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are higher number of positive ratings. Recommendations are positive i.e. the items that have positive reviews are recommended.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,axes=plt.subplots(2,4,figsize=(17,8),sharex=False)\nfor k,xvar in enumerate(['word_count','charac_count']):\n    for i,y in enumerate(['Rating','Department Name','Recommended IND']):\n        for x in set(data[y][data[y].notnull()]):\n            sns.kdeplot(data[xvar][data[y]==x],label=x, shade=False, ax=axes[k,i])\n            if k is 0:\n                axes[k,i].set_title('{} Distribution (X)\\nby {}'.format(xvar, y))\n            else:\n                axes[k,i].set_title('For {} (X)'.format(xvar))\n    axes[k,0].set_ylabel('Occurrence Density')\n    axes[k,i].set_xlabel('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Rating','Department Name','Recommended IND','word_count','charac_count']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(10, 7), sharey=True)\nfsize = 13\nsns.heatmap(pd.crosstab(data['Class Name'], data[\"Department Name\"], normalize = 'columns').mul(100).round(0)\n            ,annot=True, fmt=\"g\", linewidths=.5, ax=ax[0],cbar=False,cmap=\"Blues\")\nax[0].set_title('Class Name Count by Count - Crosstab\\nHeatmap % Distribution by Column', fontsize = fsize)\nax[1] = sns.heatmap(pd.crosstab(data['Class Name'], data[\"Department Name\"], normalize = 'index').mul(100).round(0)\n            ,annot=True, fmt=\"2g\", linewidths=.5, ax=ax[1],cmap=\"Blues\",\n                cbar_kws={'label': 'Percentage %'})\nax[1].set_title('Class Name Count by Count - Crosstab\\nHeatmap % Distribution by Index', fontsize = fsize)\nax[1].set_ylabel('')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pre-Processing\nSIA = SentimentIntensityAnalyzer()\ndata[\"Review Text\"]= data[\"Review Text\"].astype(str)\n\n# Applying Model, Variable Creation\ndata['Polarity Score']=data[\"Review Text\"].apply(lambda x:SIA.polarity_scores(x)['compound'])\ndata['Neutral Score']=data[\"Review Text\"].apply(lambda x:SIA.polarity_scores(x)['neu'])\ndata['Negative Score']=data[\"Review Text\"].apply(lambda x:SIA.polarity_scores(x)['neg'])\ndata['Positive Score']=data[\"Review Text\"].apply(lambda x:SIA.polarity_scores(x)['pos'])\n\n# Converting 0 to 1 Decimal Score to a Categorical Variable\ndata['Sentiment']=''\ndata.loc[data['Polarity Score']>0,'Sentiment']='Positive'\ndata.loc[data['Polarity Score']==0,'Sentiment']='Neutral'\ndata.loc[data['Polarity Score']<0,'Sentiment']='Negative'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def percentstandardize_barplot(x,y,hue, data, ax=None, order= None):\n    \"\"\"\n    Standardize by percentage the data using pandas functions, then plot using Seaborn.\n    Function arguments are and extention of Seaborns'.\n    \"\"\"\n    sns.barplot(x= x, y=y, hue=hue, ax=ax, order=order,\n    data=(data[[x, hue]]\n     .reset_index(drop=True)\n     .groupby([x])[hue]\n     .value_counts(normalize=True)\n     .rename('Percentage').mul(100)\n     .reset_index()\n     .sort_values(hue)))\n    plt.title(\"Percentage Frequency of {} by {}\".format(hue,x))\n    plt.ylabel(\"Percentage %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"huevar = \"Recommended IND\"\nxvar = \"Sentiment\"\nf, axes = plt.subplots(1,2,figsize=(12,5))\nsns.countplot(x=xvar, hue=huevar,data=data, ax=axes[0], order=[\"Negative\",\"Neutral\",\"Positive\"])\naxes[0].set_title(\"Occurence of {}\\nby {}\".format(xvar, huevar))\naxes[0].set_ylabel(\"Count\")\npercentstandardize_barplot(x=xvar,y=\"Percentage\", hue=huevar,data=data, ax=axes[1])\naxes[1].set_title(\"Percentage Normalized Occurence of {}\\nby {}\".format(xvar, huevar))\naxes[1].set_ylabel(\"% Percentage by {}\".format(huevar))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2,2, figsize=[9,9])\nsns.countplot(x=\"Sentiment\", data=data, ax=axes[0,0], order=[\"Negative\",\"Neutral\",\"Positive\"])\naxes[0,0].set_xlabel(\"Sentiment\")\naxes[0,0].set_ylabel(\"Count\")\naxes[0,0].set_title(\"Overall Sentiment Occurrence\")\n\nsns.countplot(x=\"Rating\", data=data, ax=axes[0,1])\naxes[0,1].set_xlabel(\"Rating\")\naxes[0,1].set_ylabel(\"\")\naxes[0,1].set_title(\"Overall Raiting Occurrence\")\n\npercentstandardize_barplot(x=\"Rating\",y=\"Percentage\",hue=\"Sentiment\",data=data, ax=axes[1,0])\naxes[1,0].set_xlabel(\"Rating\")\naxes[1,0].set_ylabel(\"Percentage %\")\naxes[1,0].set_title(\"Standardized Percentage Raiting Frequency\\nby Sentiment\")\n\npercentstandardize_barplot(x=\"Sentiment\",y=\"Percentage\",hue=\"Rating\",data=data, ax=axes[1,1])\naxes[1,1].set_ylabel(\"Occurrence Frequency\")\naxes[1,1].set_title(\"Standardized Percentage Sentiment Frequency\\nby Raiting\")\naxes[1,1].set_xlabel(\"Sentiment\")\naxes[1,1].set_ylabel(\"\")\n\nf.suptitle(\"Distribution of Sentiment Score and Rating for Customer Reviews\", fontsize=14)\nf.tight_layout()\nf.subplots_adjust(top=0.92)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tweakable Variables (Note to Change Order Arguement if Xvar is changed)\nxvar = \"Sentiment\"\nhuevar = \"Department Name\"\nrowvar = \"Recommended IND\"\n\n# Plot\nf, axes = plt.subplots(2,2,figsize=(10,10), sharex=False,sharey=False)\nfor i,x in enumerate(set(data[rowvar][data[rowvar].notnull()])):\n    percentstandardize_barplot(x=xvar,y=\"Percentage\", hue=huevar,data=data[data[rowvar] == x],\n                 ax=axes[i,0], order=[\"Negative\",\"Neutral\",\"Positive\"])\n    percentstandardize_barplot(x=xvar,y=\"Percentage\", hue=\"Rating\",data=data[data[rowvar] == x],\n                 ax=axes[i,1], order=[\"Negative\",\"Neutral\",\"Positive\"])\n\n# Plot Aesthetics\naxes[1,0].legend_.remove()\naxes[1,1].legend_.remove()\naxes[0,1].set_ylabel(\"\")\naxes[1,1].set_ylabel(\"\")\naxes[0,0].set_xlabel(\"\")\naxes[0,1].set_xlabel(\"\")\naxes[0,0].set_ylabel(\"Recommended = FALSE\\nPercentage %\")\naxes[1,0].set_ylabel(\"Recommended = TRUE\\nPercentage %\")\naxes[1,1].set_title(\"\")\n\n# Common title and ylabel\nf.text(0.0, 0.5, 'Subplot Rows\\nSliced by Recommended', va='center', rotation='vertical', fontsize=12)\nf.suptitle(\"Review Sentiment by Department Name and Raiting\\nSubplot Rows Slice Data by Recommended\", fontsize=16)\nf.tight_layout()\nf.subplots_adjust(top=0.93)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Correlation Matrix\nf, ax = plt.subplots(figsize=[9,6])\nax = sns.heatmap(data.corr(), annot=True,\n                 fmt=\".2f\",cbar_kws={'label': 'Correlation Coefficient'})\nax.set_title(\"Correlation Matrix for All Variables\")\nplt.show()\n\n# Sentiment Positivity Score by Positive Feedback Count\nax = sns.jointplot(x= data[\"Positive Feedback Count\"], y=data[\"Positive Score\"], kind='reg', color='r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)\nsize = (10,7)\n\ndef cloud(text, title, stopwords=stopwords, size=size):\n    \"\"\"\n    Function to plot WordCloud\n    Includes: \n    \"\"\"\n    # Setting figure parameters\n    mpl.rcParams['figure.figsize']=(10.0,10.0)\n    mpl.rcParams['font.size']=12\n    mpl.rcParams['savefig.dpi']=100\n    mpl.rcParams['figure.subplot.bottom']=.1 \n    \n    # Processing Text\n    # Redundant when combined with my Preprocessing function\n    wordcloud = WordCloud(width=1600, height=800,\n                          background_color='black',\n                          stopwords=stopwords,\n                         ).generate(str(text))\n    \n    # Output Visualization\n    fig = plt.figure(figsize=size, dpi=80, facecolor='k',edgecolor='k')\n    plt.imshow(wordcloud,interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title, fontsize=50,color='y')\n    plt.tight_layout(pad=0)\n    plt.show()\n    \n# Frequency Calculation [One-Gram]\ndef wordfreqviz(text, x):\n    word_dist = nltk.FreqDist(text)\n    top_N = x\n    rslt = pd.DataFrame(word_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word')\n    matplotlib.style.use('ggplot')\n    rslt.plot.bar(rot=0)\n\ndef wordfreq(text, x):\n    word_dist = nltk.FreqDist(text)\n    top_N = x\n    rslt = pd.DataFrame(word_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word')\n    return rslt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modify Stopwords to Exclude Class types, suchs as \"dress\"\nnew_stop = set(STOPWORDS)\nnew_stop.update([x.lower() for x in list(data[\"Class Name\"][data[\"Class Name\"].notnull()].unique())]\n                + [\"dress\", \"petite\"])\n\n# Cloud\ncloud(text= data.Title[data.Title.notnull()].astype(str).values,\n      title=\"Titles\",\n      stopwords= new_stop,\n      size = (7,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Highly Raited\ntitle =\"Highly Rated Comments\"\ntemp = data['Review Text'][data.Rating.astype(int) >= 3]\n\n# Modify Stopwords to Exclude Class types, suchs as \"dress\"\nnew_stop = set(STOPWORDS)\nnew_stop.update([x.lower() for x in list(data[\"Class Name\"][data[\"Class Name\"].notnull()].unique())]\n                + [\"dress\", \"petite\"])\n\n# Cloud\ncloud(text= temp.values, title=title,stopwords= new_stop)\n\n# Bar Chart\nwordfreq(preprocessing(temp),20).plot.bar(rot=45, legend=False,figsize=(15,5), color='g',\n                          title= title)\nplt.ylabel(\"Occurrence Count\")\nplt.xlabel(\"Most Frequent Words\")\nplt.show()\n\n# Low Raited\ntitle =\"Most Frequent Words in Low Rated Comments\"\ntemp = data['Review Text'][data.Rating.astype(int) < 3]\n\n# Modify Stopwords to Exclude Class types, suchs as \"dress\"\nnew_stop = set(STOPWORDS)\nnew_stop.update([x.lower() for x in list(data[\"Class Name\"][data[\"Class Name\"].notnull()].unique())]\n                + [\"dress\", \"petite\", \"skirt\",\"shirt\"])\n\n# Cloud\ncloud(temp.values, title= title, stopwords = new_stop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"department_set = data[\"Department Name\"][data[\"Department Name\"].notnull()].unique()\ndivision_set = data[\"Division Name\"][data[\"Division Name\"].notnull()].unique()\ndef cloud_by_category(data, category, subclass):\n    \"\"\"\n    Function to create a wordcloud by class and subclass\n    Category signifies the column variable\n    Subclass refers to the specific value within the categorical variable\n    \"\"\"\n    new_stop = set(STOPWORDS)\n    new_stop.update([x.lower() for x in list(data[\"Class Name\"][data[\"Class Name\"].notnull()].unique())]\n                   + [x.lower() for x in list(data[\"Department Name\"][data[\"Department Name\"].notnull()].unique())]\n                   + [\"dress\", \"petite\", \"jacket\",\"top\"])\n\n    # Cloud\n    cloud(text= data[\"Review Text\"][data[category]== subclass],\n          title=\"{}\".format(subclass),\n          stopwords= new_stop,\n          size = (10,6))\n    \n# Plot\ncloud_by_category(data, \"Division Name\", division_set[0])\ncloud_by_category(data, \"Division Name\", division_set[1])\ncloud_by_category(data, \"Division Name\", division_set[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Helper Functions\nfrom nltk.util import ngrams\nfrom collections import Counter\ndef get_ngrams(text, n):\n    n_grams = ngrams((text), n)\n    return [ ' '.join(grams) for grams in n_grams]\n\ndef gramfreq(text,n,num):\n    # Extracting bigrams\n    result = get_ngrams(text,n)\n    # Counting bigrams\n    result_count = Counter(result)\n    # Converting to the result to a data frame\n    data = pd.DataFrame.from_dict(result_count, orient='index')\n    data = data.rename(columns={'index':'words', 0:'frequency'}) # Renaming index column name\n    return df.sort_values([\"frequency\"],ascending=[0])[:num]\n\ndef gram_table(data, gram, length):\n    out = pd.DataFrame(index=None)\n    for i in gram:\n        table = pd.DataFrame(gramfreq(preprocessing(data),i,length).reset_index())\n        table.columns = [\"{}-Gram\".format(i),\"Occurrence\"]\n        out = pd.concat([out, table], axis=1)\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Non-Recommended Items\")\ngram_table(data= data['Review Text'][data[\"Recommended IND\"].astype(int) == 0], gram=[1,2,3,4,5], length=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Recommended Items\")\ngram_table(data= data['Review Text'][data[\"Recommended IND\"].astype(int) == 1], gram=[1,2,3,4,5], length=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['tokenized'] = data[\"Review Text\"].astype(str).str.lower() # Turn into lower case text\ndata['tokenized'] = data.apply(lambda row: tokenizer.tokenize(row['tokenized']), axis=1) # Apply tokenize to each row\ndata['tokenized'] = data['tokenized'].apply(lambda x: [w for w in x if not w in stop_words]) # Remove stopwords from each row\ndata['tokenized'] = data['tokenized'].apply(lambda x: [ps.stem(w) for w in x]) # Apply stemming to each row\nall_words = nltk.FreqDist(preprocessing(data['Review Text'])) # Calculate word occurrence from whole block of text\n\nvocab_count = 200\nword_features= list(all_words.keys())[:vocab_count] # 2000 most recurring unique words\nprint(\"Number of words columns (One Hot Encoding): {}\".format(len(all_words)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport scikitplot as skplt\nimport eli5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = TfidfVectorizer()\nvect.fit(data[\"Review Text\"])\nX = vect.transform(data[\"Review Text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"Recommended IND\"].copy()\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.20, random_state=23, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nprint(\"Train Set Accuracy: {}\".format(metrics.accuracy_score(model.predict(X_train), y_train)))\nprint(\"Train Set ROC: {}\\n\".format(metrics.roc_auc_score(model.predict(X_train), y_train)))\n\nprint(\"Validation Set Accuracy: {}\".format(metrics.accuracy_score(model.predict(X_valid), y_valid)))\nprint(\"Validation Set ROC: {}\".format(metrics.roc_auc_score(model.predict(X_valid), y_valid)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(model.predict(X_valid), y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\nskplt.metrics.plot_confusion_matrix(model.predict(X_valid), y_valid, normalize=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_names = [\"Not Recommended\",\"Recommended\"]\neli5.show_weights(model, vec=vect, top=100,\n                  target_names=target_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for iteration in range(15):\n    samp = random.randint(1,data.shape[0])\n    print(\"Real Label: {}\".format(data[\"Recommended IND\"].iloc[samp]))\n    display(eli5.show_prediction(model,data[\"Review Text\"].iloc[samp], vec=vect,\n                         target_names=target_names))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}